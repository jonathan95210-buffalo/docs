---
title: PyTorch
description: Integrate PyTorch code with W&B to add experiment tracking
---
import ColabLink from "/snippets/en/_includes/colab-link.mdx";

<ColabLink url="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb" />

Use Weights & Biases [W&B](https://wandb.ai) for machine learning experiment tracking, dataset versioning, and project collaboration.

<Frame>
    <img src="/images/tutorials/huggingface-why.png" alt="Benefits of using W&B"  />
</Frame>

## What this notebook covers

This notebook shows you how to integrate W&B with your PyTorch code to add experiment tracking to your pipeline.

<Frame>
    <img src="/images/tutorials/pytorch.png" alt="PyTorch and W&B integration diagram"  />
</Frame>

The above image shows a successful integration of PyTorch code into a W&B pipeline.

```python
# Import the library
import wandb

# Capture a dictionary of hyperparameters with config
config = {
    "learning_rate": 0.001,
    "epochs": 100,
    "batch_size": 128
}

# Start a new experiment
with wandb.init(project="new-sota-model", config=config) as run:

    # Set up model and data
    model, dataloader = get_model(), get_data()

    # Track gradients (optional)
    run.watch(model)

    for batch in dataloader:
    metrics = model.training_step()
    # Log metrics inside your training loop to visualize model performance
    run.log(metrics)

    # Save model (optional)
    model.to_onnx()
    run.save("model.onnx")
```

For a video tutorial on using PyTorch, see [Integrate Weights & Biases with PyTorch](https://wandb.me/pytorch-video).

**Note**: Sections starting with **Step** are all you need to integrate W&B into an existing pipeline. The remaining sections explain how to load data and define a model.

## Install, import, and log in


```python
import os
import random

import numpy as np
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from tqdm.auto import tqdm

# Ensure deterministic behavior
torch.backends.cudnn.deterministic = True
random.seed(hash("setting random seeds") % 2**32 - 1)
np.random.seed(hash("improves reproducibility") % 2**32 - 1)
torch.manual_seed(hash("by removing stochasticity") % 2**32 - 1)
torch.cuda.manual_seed_all(hash("so runs are repeatable") % 2**32 - 1)

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Remove slow mirror from list of MNIST mirrors
torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors
                                      if not mirror.startswith("http://yann.lecun.com")]
```

### Step 0: Install W&B

To get started, install the `wandb` library using `pip`.


```python
!pip install wandb onnx -Uq
```

### Step 1: Import W&B and log in

To log data to the W&B web service, you need to log in.

If this is your first time using W&B, you will need to sign up for a free account on the link that appears.

```python
import wandb

wandb.login()
```

## Define the experiment and pipeline

### Track metadata and hyperparameters with `wandb.init`

The first step is to define the experiment.
What are the hyperparameters? What metadata is associated with this run?

A common workflow is to store this information in a `config` dictionary
(or similar object) and then access it as needed. For this example, only a few
hyperparameters vary and you hand-code the rest.

For this example, only a few hyperparameters vary and you hand-code the rest.
Any part of your model can be part of the `config`.

Some metadata is included. For this example, we use the MNIST dataset and a 
convolutional architecture. In the future, if fully connected architectures is 
used on CIFAR in the same project, it will help to separate the runs.

```python
config = dict(
    epochs=5,
    classes=10,
    kernels=[16, 32],
    batch_size=128,
    learning_rate=0.005,
    dataset="MNIST",
    architecture="CNN")
```

Next, define the overall pipeline,
one that is common for model-training:

1. `make` a model, plus associated data and optimizer
1. `train` the model accordingly
1. `test` the model to evaluate the training step

These functions are implemented below.

```python
def model_pipeline(hyperparameters):

    # Initialize wandb 
    with wandb.init(project="pytorch-demo", config=hyperparameters) as run:
        # Access all hyperparamters through run.config, so that logging matches execution
        config = run.config

        # Make the model, data, and optimization problem
        model, train_loader, test_loader, criterion, optimizer = make(config)
        print(model)

        # Train the model on it
        train(model, train_loader, criterion, optimizer, config)

        # Test its final performance
        test(model, test_loader)

    return model
```

The only difference from a standard pipeline
is that it all occurs inside the context of `wandb.init`.
Calling this function sets up a line of communication
between your code and our servers.

Passing the `config` dictionary to `wandb.init` immediately logs the data,
which ensures that you can always validate what hyperparameter values
you set your experiment to use.

To ensure the values you chose and logged are always the ones that get used
in your model, we recommend using the `run.config` copy of your object.
Check the definition of `make` below to see some examples.

> **Note**: We independently run our code in separate processes,
so that any issues on our end (such as network issues at data centers)
don't cause an application failure.

Once the issue is resolved, you can log the data with `wandb sync`.


```python
def make(config):
    # Make the data
    train, test = get_data(train=True), get_data(train=False)
    train_loader = make_loader(train, batch_size=config.batch_size)
    test_loader = make_loader(test, batch_size=config.batch_size)

    # Make the model
    model = ConvNet(config.kernels, config.classes).to(device)

    # Make the loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(
        model.parameters(), lr=config.learning_rate)
    
    return model, train_loader, test_loader, criterion, optimizer
```

### Define the data loading and model

Specify how the data is loaded and what the model looks like.

This part is very important, but it's no different that what the 
alternative would be without  `wandb`.

```python
def get_data(slice=5, train=True):
    full_dataset = torchvision.datasets.MNIST(root=".",
                                              train=train, 
                                              transform=transforms.ToTensor(),
                                              download=True)
    #  Equivalent to slicing with [::slice] 
    sub_dataset = torch.utils.data.Subset(
      full_dataset, indices=range(0, len(full_dataset), slice))
    
    return sub_dataset


def make_loader(dataset, batch_size):
    loader = torch.utils.data.DataLoader(dataset=dataset,
                                         batch_size=batch_size, 
                                         shuffle=True,
                                         pin_memory=True, num_workers=2)
    return loader
```

Defining the model is normally enjoyable and nothing changes with `wandb`.
We use a standard ConvNet architecture. This is a good opportunity to try some experiments.
All your results will be logged on [wandb.ai](https://wandb.ai).

```python
# Convolutional neural network

class ConvNet(nn.Module):
    def __init__(self, kernels, classes=10):
        super(ConvNet, self).__init__()
        
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, kernels[1], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out
```

### Define training logic

The `model_pipeline` requires that we specify how we `train` the model.
Two `wandb` functions used are `watch` and `log`.

## Track gradients with `run.watch()` and everything else with `run.log()`

`run.watch` will log the gradients and the parameters of your model,
every `log_freq` steps of training.

Call this function before you start the training loop.

The remainder of the training code follows standard practices
and iterates over epochs and batches, runs forward and backwards
passes, and applies the `optimizer`.

```python
def train(model, loader, criterion, optimizer, config):
    # Track the model gradients, weights, and parameters with wandb.
    run = wandb.init(project="pytorch-demo", config=config)
    run.watch(model, criterion, log="all", log_freq=10)

    # Run training and track with wandb
    total_batches = len(loader) * config.epochs
    example_ct = 0  # number of examples seen
    batch_ct = 0
    for epoch in tqdm(range(config.epochs)):
        for _, (images, labels) in enumerate(loader):

            loss = train_batch(images, labels, model, optimizer, criterion)
            example_ct +=  len(images)
            batch_ct += 1

            # Report metrics every 25th batch
            if ((batch_ct + 1) % 25) == 0:
                train_log(loss, example_ct, epoch)


def train_batch(images, labels, model, optimizer, criterion):
    images, labels = images.to(device), labels.to(device)
    
    # Forward pass ➡
    outputs = model(images)
    loss = criterion(outputs, labels)
    
    # Backward pass ⬅
    optimizer.zero_grad()
    loss.backward()

    # Step with optimizer
    optimizer.step()

    return loss
```

The only difference is in the logging code. Previously, you might have
reported metrics by printing to the terminal, now you pass the same information to `run.log()`.

`run.log()` expects a dictionary with string keys.
These strings identify the objects being logged, which make up the values.
You can also optionally log which `step` of training you're on.

> **Note**: Use the number of examples the model has seen, since this shows you
easier comparison across batch sizes. You can use raw steps or batch count.
For longer training runs, it might make sense to log by `epoch`.


```python
def train_log(loss, example_ct, epoch):
    with wandb.init(project="pytorch-demo") as run:
        # Log the loss and epoch number to W&B
        run.log({"epoch": epoch, "loss": loss}, step=example_ct)
        print(f"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}")
```

### Define testing logic

After training completes, you should test it. Run it against recent data from 
productions, or apply it to some hand-curated examples.

## Call `run.save()` (optional)

This is also a great time to save the model's architecture
and final parameters to disk.
For maximum compatibility, we'll `export` our model in the
[Open Neural Network eXchange (ONNX) format](https://onnx.ai/).

Passing that filename to `run.save()` ensures that the model parameters
are saved to W&B's servers: no more losing track of which `.h5` or `.pb`
corresponds to which training runs.

For more advanced `wandb` features for storing, versioning, and distributing
models, check out our [Artifacts tools](https://www.wandb.com/artifacts).


```python
def test(model, test_loader):
    model.eval()

    with wandb.init(project="pytorch-demo") as run:
        # Run the model on some test examples
        with torch.no_grad():
            correct, total = 0, 0
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

            print(f"Accuracy of the model on the {total} " +
                f"test images: {correct / total:%}")
            
            run.log({"test_accuracy": correct / total})

        # Save the model in the exchangeable ONNX format
        torch.onnx.export(model, images, "model.onnx")
        run.save("model.onnx")
```

### Run training and view metrics on wandb.ai

After defining the pipeline and adding W&B code, you can run a fully tracked experiment.

These links are shared with you:
- The documentation
- The Project page, which organizes all the runs in a project
- The Run page, where the run's results are stored

Navigate to the Run page and check out these tabs:

1. **Charts**: The model gradients, parameter values, and loss are logged throughout training
2. **System**: System metrics including Disk I/O utilization, CPU and GPU metrics
3. **Logs**: A copy of anything pushed to standard out during training
4. **Files**: After training completes, click `model.onnx` to view our network with the [Netron model viewer](https://github.com/lutzroeder/netron).

Once the run is finished, and the `with wandb.init` block exits,
A summary of the results are printed in the cell output.

```python
# Build, train and analyze the model with the pipeline
model = model_pipeline(config)
```

### Test hyperparameters with sweeps

This example uses a single set of hyperparameters However, an important part of most 
ML workflows is iterating over multiple hyperparameters.

You can use W&B Sweeps to automate hyperparameter testing and explore the space of possible models and optimization strategies.

Review a [Colab notebook demonstrating hyperparameter optimization using W&B Sweeps](https://wandb.me/sweeps-colab).

Running a hyperparameter sweep with W&B is simple. Follow these simple steps:

1. **Define the sweep**: Create a dictionary or a [YAML file](/models/sweeps/define-sweep-configuration/) that specifies:
- Parameters to search
- Search strategy
- Optimization metric
- Other sweep configurations

2. **Initialize the sweep**: 

```python
sweep_id = wandb.sweep(sweep_config)
```

3. **Run the sweep agent**: 

```python
wandb.agent(sweep_id, function=train)
```

The previous steps explain how to run a hyperparameter sweep.

<Frame>
    <img src="/images/tutorials/pytorch-2.png" alt="PyTorch training dashboard"  />
</Frame>

This image of the PyTorch training dashboard shows more details.

## Example gallery

Explore examples of projects tracked and visualized with W&B in the [Gallery →](https://app.wandb.ai/gallery).

## Advanced setup
1. [Environment variables](/platform/hosting/env-vars/): Set API keys in environment variables to run training on a managed cluster.
2. [Offline mode](/models/support/run_wandb_offline/): Use `dryrun` mode to train offline and sync results later.
3. [On-prem](/platform/hosting/hosting-options/self-managed): Install W&B in a private cloud or air-gapped servers in your own infrastructure. Local installations are available for academics to enterprise teams.
4. [Sweeps](/models/sweeps/): Set up a hyperparameter search quickly with our lightweight tuning tool.
